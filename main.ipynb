{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1033a73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CUDA Verification\n",
      "==================================================\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 90100\n",
      "Number of GPUs: 1\n",
      "\n",
      "GPU Details:\n",
      "  GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "    Memory: 4.00 GB\n",
      "\n",
      "Testing GPU computation...\n",
      "‚úì GPU computation successful!\n",
      "  Result tensor shape: torch.Size([1000, 1000])\n",
      "  Result tensor device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verify CUDA installation\n",
    "print(\"=\" * 50)\n",
    "print(\"CUDA Verification\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version() if torch.cuda.is_available() else 'N/A'}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 0}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Details:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Quick test: create a tensor on GPU\n",
    "    print(f\"\\nTesting GPU computation...\")\n",
    "    x = torch.randn(1000, 1000).cuda()\n",
    "    y = torch.randn(1000, 1000).cuda()\n",
    "    z = torch.matmul(x, y)\n",
    "    print(f\"‚úì GPU computation successful!\")\n",
    "    print(f\"  Result tensor shape: {z.shape}\")\n",
    "    print(f\"  Result tensor device: {z.device}\")\n",
    "else:\n",
    "    print(\"\\n‚ö† CUDA is not available. Check your installation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7238b7",
   "metadata": {},
   "source": [
    "# Model Training Setup\n",
    "\n",
    "Before we can use the model for predictions and interpretability, we need to train it on our art dataset.\n",
    "\n",
    "## Training Strategy:\n",
    "1. **Select subset of artists**: Take half of all available artists\n",
    "2. **Split data**: Use 70% of selected artists' paintings for training, 15% for validation, 15% for testing\n",
    "3. **Train CNN model**: Based on CIFAR-10 architecture, adapted for art classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3597e",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Preparation and Artist Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33744111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bd95c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for art paintings\n",
    "class ArtDataset(Dataset):\n",
    "    def __init__(self, root_dir, artists=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Root directory containing artist folders\n",
    "            artists: List of artist names to include (None = all artists)\n",
    "            transform: Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # If artists not specified, discover all artist folders\n",
    "        if artists is None:\n",
    "            self.artists = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        else:\n",
    "            self.artists = artists\n",
    "        \n",
    "        # Create artist to index mapping\n",
    "        self.artist_to_idx = {artist: idx for idx, artist in enumerate(self.artists)}\n",
    "        self.idx_to_artist = {idx: artist for artist, idx in self.artist_to_idx.items()}\n",
    "        \n",
    "        # Load all image paths with their labels\n",
    "        self.samples = []\n",
    "        for artist in self.artists:\n",
    "            artist_dir = self.root_dir / artist\n",
    "            if artist_dir.exists():\n",
    "                # Support common image formats\n",
    "                image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "                for img_path in artist_dir.iterdir():\n",
    "                    if img_path.suffix.lower() in image_extensions:\n",
    "                        self.samples.append((str(img_path), self.artist_to_idx[artist]))\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} images from {len(self.artists)} artists\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image (you may need to install Pillow: pip install Pillow)\n",
    "        from PIL import Image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            # Note: Original image size may vary, but transform will resize to 224x224\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        # Apply transforms (which includes resizing to 224x224)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def get_artist_counts(self):\n",
    "        \"\"\"Get count of images per artist\"\"\"\n",
    "        counts = defaultdict(int)\n",
    "        for _, label in self.samples:\n",
    "            artist = self.idx_to_artist[label]\n",
    "            counts[artist] += 1\n",
    "        return dict(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20e7a5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transforms configured for ResNet (224x224 input)\n",
      "  Training: Enhanced augmentation (rotation, color jitter, random crop)\n",
      "  Test: Center crop only (no augmentation)\n"
     ]
    }
   ],
   "source": [
    "# Define data transforms\n",
    "# IMPORTANT: All images will be resized to 224x224 regardless of original size\n",
    "# ResNet expects 224x224 input (ImageNet standard)\n",
    "TARGET_SIZE = (224, 224)\n",
    "\n",
    "# Training transforms: Enhanced augmentation for art classification\n",
    "# Following best practices for transfer learning with ResNet\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize to slightly larger first\n",
    "    transforms.RandomCrop(TARGET_SIZE),  # Random crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flip\n",
    "    transforms.RandomRotation(15),  # Rotation up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),  # Color augmentation\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Slight translation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Validation/Test transforms: No augmentation, just resize and normalize\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize to slightly larger\n",
    "    transforms.CenterCrop(TARGET_SIZE),  # Center crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "print(f\"‚úÖ Transforms configured for ResNet (224x224 input)\")\n",
    "print(f\"  Training: Enhanced augmentation (rotation, color jitter, random crop)\")\n",
    "print(f\"  Test: Center crop only (no augmentation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55189d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8774 images from 51 artists\n",
      "\n",
      "üìä Found 51 artists in dataset:\n",
      "  Vincent_van_Gogh: 877 images\n",
      "  Edgar_Degas: 702 images\n",
      "  Pablo_Picasso: 439 images\n",
      "  Pierre-Auguste_Renoir: 336 images\n",
      "  Albrecht_DuŒì√≤√°‚îú¬¨rer: 328 images\n",
      "  Albrecht_Du‚ï†√™rer: 328 images\n",
      "  Paul_Gauguin: 311 images\n",
      "  Francisco_Goya: 291 images\n",
      "  Rembrandt: 262 images\n",
      "  Alfred_Sisley: 259 images\n",
      "  Titian: 255 images\n",
      "  Marc_Chagall: 239 images\n",
      "  Rene_Magritte: 194 images\n",
      "  Amedeo_Modigliani: 193 images\n",
      "  Paul_Klee: 188 images\n",
      "  Henri_Matisse: 186 images\n",
      "  Andy_Warhol: 181 images\n",
      "  Mikhail_Vrubel: 171 images\n",
      "  Sandro_Botticelli: 164 images\n",
      "  Leonardo_da_Vinci: 143 images\n",
      "  Peter_Paul_Rubens: 141 images\n",
      "  Salvador_Dali: 139 images\n",
      "  Hieronymus_Bosch: 137 images\n",
      "  Pieter_Bruegel: 134 images\n",
      "  Diego_Velazquez: 128 images\n",
      "  Kazimir_Malevich: 126 images\n",
      "  Frida_Kahlo: 120 images\n",
      "  Giotto_di_Bondone: 119 images\n",
      "  Gustav_Klimt: 117 images\n",
      "  Raphael: 109 images\n",
      "  Joan_Miro: 102 images\n",
      "  Andrei_Rublev: 99 images\n",
      "  Camille_Pissarro: 91 images\n",
      "  Edouard_Manet: 90 images\n",
      "  Vasiliy_Kandinskiy: 88 images\n",
      "  El_Greco: 87 images\n",
      "  Piet_Mondrian: 84 images\n",
      "  Henri_de_Toulouse-Lautrec: 81 images\n",
      "  Jan_van_Eyck: 81 images\n",
      "  Claude_Monet: 73 images\n",
      "  Diego_Rivera: 70 images\n",
      "  Henri_Rousseau: 70 images\n",
      "  Edvard_Munch: 67 images\n",
      "  William_Turner: 66 images\n",
      "  Gustave_Courbet: 59 images\n",
      "  Caravaggio: 55 images\n",
      "  Michelangelo: 49 images\n",
      "  Paul_Cezanne: 47 images\n",
      "  Georges_Seurat: 43 images\n",
      "  Eugene_Delacroix: 31 images\n",
      "  Jackson_Pollock: 24 images\n",
      "\n",
      "üé® Selected 25 artists for training:\n",
      "  - Albrecht_DuŒì√≤√°‚îú¬¨rer\n",
      "  - Albrecht_Du‚ï†√™rer\n",
      "  - Alfred_Sisley\n",
      "  - Amedeo_Modigliani\n",
      "  - Andrei_Rublev\n",
      "  - Andy_Warhol\n",
      "  - Camille_Pissarro\n",
      "  - Caravaggio\n",
      "  - Claude_Monet\n",
      "  - Diego_Rivera\n",
      "  - Diego_Velazquez\n",
      "  - Edgar_Degas\n",
      "  - Edouard_Manet\n",
      "  - Edvard_Munch\n",
      "  - El_Greco\n",
      "  - Eugene_Delacroix\n",
      "  - Francisco_Goya\n",
      "  - Frida_Kahlo\n",
      "  - Georges_Seurat\n",
      "  - Giotto_di_Bondone\n",
      "  - Gustav_Klimt\n",
      "  - Gustave_Courbet\n",
      "  - Henri_Matisse\n",
      "  - Henri_Rousseau\n",
      "  - Henri_de_Toulouse-Lautrec\n",
      "Loaded 3868 images from 25 artists\n",
      "Loaded 3868 images from 25 artists\n",
      "Loaded 3868 images from 25 artists\n",
      "\n",
      "üì¶ Dataset splits:\n",
      "  Training: 2707 images (70.0%)\n",
      "  Test: 1161 images (30.0%)\n",
      "\n",
      "‚úÖ Dataset ready! Number of classes: 25\n",
      "\n",
      "üîç Verifying image resizing...\n",
      "  Sample batch shape: torch.Size([32, 3, 224, 224])\n",
      "  Expected shape: [batch_size, 3, 224, 224]\n",
      "  ‚úÖ All images are correctly resized to 224x224!\n"
     ]
    }
   ],
   "source": [
    "# Load full dataset\n",
    "# TODO: Update this path to your actual dataset directory\n",
    "# Expected structure: dataset_root/artist_name/image1.jpg, image2.jpg, ...\n",
    "DATASET_ROOT = \"art Folder/images/images\"  # CHANGE THIS!\n",
    "import os\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(DATASET_ROOT):\n",
    "    print(f\"‚ö†Ô∏è  Dataset not found at: {DATASET_ROOT}\")\n",
    "    print(\"Please update DATASET_ROOT with your actual dataset path\")\n",
    "    print(\"\\nExpected directory structure:\")\n",
    "    print(\"dataset_root/\")\n",
    "    print(\"  ‚îú‚îÄ‚îÄ Picasso/\")\n",
    "    print(\"  ‚îÇ   ‚îú‚îÄ‚îÄ painting1.jpg\")\n",
    "    print(\"  ‚îÇ   ‚îî‚îÄ‚îÄ painting2.jpg\")\n",
    "    print(\"  ‚îú‚îÄ‚îÄ Matisse/\")\n",
    "    print(\"  ‚îÇ   ‚îî‚îÄ‚îÄ ...\")\n",
    "    print(\"  ‚îî‚îÄ‚îÄ ...\")\n",
    "else:\n",
    "    # 1) Load all artists from the full dataset (no transform yet)\n",
    "    full_dataset = ArtDataset(DATASET_ROOT, transform=None)\n",
    "    all_artists = full_dataset.artists\n",
    "\n",
    "    print(f\"\\nüìä Found {len(all_artists)} artists in dataset:\")\n",
    "    artist_counts = full_dataset.get_artist_counts()\n",
    "    for artist, count in sorted(artist_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {artist}: {count} images\")\n",
    "\n",
    "    # 2) Select half of the artists (you can change this strategy if you like)\n",
    "    num_artists_to_use = len(all_artists) // 2\n",
    "    selected_artists = sorted(all_artists)[:num_artists_to_use]\n",
    "\n",
    "    print(f\"\\nüé® Selected {len(selected_artists)} artists for training:\")\n",
    "    for artist in selected_artists:\n",
    "        print(f\"  - {artist}\")\n",
    "\n",
    "    # 3) Base dataset restricted to selected artists (no transform yet)\n",
    "    base_dataset = ArtDataset(DATASET_ROOT, artists=selected_artists, transform=None)\n",
    "\n",
    "    # 4) Split into train (70%), test (30%)\n",
    "    total_size = len(base_dataset)\n",
    "    train_size = int(0.70 * total_size)\n",
    "    test_size = total_size - train_size  # Remaining 30% goes to test\n",
    "\n",
    "    train_subset, test_subset = random_split(\n",
    "        base_dataset,\n",
    "        [train_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # Extract indices from the subsets\n",
    "    train_indices = train_subset.indices\n",
    "    test_indices = test_subset.indices\n",
    "\n",
    "    # 5) Rebuild datasets with different transforms for each split\n",
    "    #    (we assume train_transform and val_test_transform are defined earlier)\n",
    "    train_dataset = torch.utils.data.Subset(\n",
    "        ArtDataset(DATASET_ROOT, artists=selected_artists, transform=train_transform),\n",
    "        train_indices\n",
    "    )\n",
    "    test_dataset = torch.utils.data.Subset(\n",
    "        ArtDataset(DATASET_ROOT, artists=selected_artists, transform=val_test_transform),\n",
    "        test_indices\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüì¶ Dataset splits:\")\n",
    "    print(f\"  Training: {len(train_dataset)} images ({len(train_dataset)/total_size*100:.1f}%)\")\n",
    "    print(f\"  Test: {len(test_dataset)} images ({len(test_dataset)/total_size*100:.1f}%)\")\n",
    "\n",
    "    # 6) Create data loaders\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # use 0 to avoid multiprocessing issues while debugging\n",
    "        pin_memory=(device.type == \"cuda\")\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=(device.type == \"cuda\")\n",
    "    )\n",
    "\n",
    "    num_classes = len(selected_artists)\n",
    "    print(f\"\\n‚úÖ Dataset ready! Number of classes: {num_classes}\")\n",
    "\n",
    "    # 7) Verify image sizes are correct\n",
    "    print(\"\\nüîç Verifying image resizing...\")\n",
    "    sample_image, sample_label = next(iter(train_loader))\n",
    "    print(f\"  Sample batch shape: {sample_image.shape}\")\n",
    "    print(\"  Expected shape: [batch_size, 3, 224, 224]\")\n",
    "    if sample_image.shape[2] == 224 and sample_image.shape[3] == 224:\n",
    "        print(\"  ‚úÖ All images are correctly resized to 224x224!\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Warning: Images are not 224x224! \"\n",
    "              f\"Actual size: {sample_image.shape[2]}x{sample_image.shape[3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba05fa",
   "metadata": {},
   "source": [
    "## Model Architecture: Transfer Learning with ResNet50\n",
    "\n",
    "**Improvements for Better Accuracy:**\n",
    "\n",
    "1. **Transfer Learning**: Using pretrained ResNet50 (trained on ImageNet) instead of training from scratch\n",
    "   - ResNet50 has learned rich visual features that transfer well to art classification\n",
    "   - Much faster convergence and higher accuracy\n",
    "\n",
    "2. **Enhanced Data Augmentation**:\n",
    "   - Random crop, rotation, color jitter, and translation\n",
    "   - Helps model generalize better to different art styles and orientations\n",
    "\n",
    "3. **Differential Learning Rates**:\n",
    "   - Lower LR (0.0001) for pretrained backbone layers\n",
    "   - Higher LR (0.001) for new classifier layers\n",
    "   - Allows fine-tuning without destroying pretrained features\n",
    "\n",
    "4. **Better Architecture**:\n",
    "   - ResNet50 backbone (50 layers, 2048 features)\n",
    "   - Custom classifier with embedding layer for similarity search\n",
    "   - Dropout for regularization\n",
    "\n",
    "This approach follows best practices from art classification research and should significantly improve accuracy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283ffe3",
   "metadata": {},
   "source": [
    "## Step 2: Model Architecture\n",
    "\n",
    "Based on CIFAR-10 tutorial, adapted for art classification with embedding support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85a94da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model initialized with 25 classes\n",
      "Model parameters: 24,694,873\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ArtClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Art classification model using transfer learning with ResNet50\n",
    "    Based on best practices for art classification tasks\n",
    "    Modified to support embedding extraction for similarity search\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, use_pretrained=True):\n",
    "        super(ArtClassifier, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet50\n",
    "        resnet = models.resnet50(pretrained=use_pretrained)\n",
    "        \n",
    "        # Freeze early layers for transfer learning (optional - can unfreeze later)\n",
    "        # for param in list(resnet.parameters())[:-10]:\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # Replace the final fully connected layer\n",
    "        # ResNet50's fc layer expects 2048 features (from avgpool)\n",
    "        num_features = resnet.fc.in_features\n",
    "        \n",
    "        # Remove the original classifier\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # Remove final fc layer\n",
    "        \n",
    "        # Add custom classifier with embedding layer\n",
    "        self.fc1 = nn.Linear(num_features, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)  # Embedding layer for similarity search\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(256, num_classes)  # Final classification layer\n",
    "        \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        # Extract features using ResNet backbone\n",
    "        x = self.backbone(x)\n",
    "        # Flatten: ResNet avgpool outputs [batch_size, 2048, 1, 1]\n",
    "        x = x.view(x.size(0), -1)  # [batch_size, 2048]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        embedding = F.relu(self.fc2(x))  # Embedding for similarity search\n",
    "        x = self.dropout2(embedding)\n",
    "        logits = self.fc3(x)  # Classification logits\n",
    "        \n",
    "        if return_embedding:\n",
    "            return logits, embedding\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "if 'num_classes' in locals():\n",
    "    model = ArtClassifier(num_classes=num_classes).to(device)\n",
    "    print(f\"\\n‚úÖ Model initialized with {num_classes} classes\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the dataset preparation cell first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ff43e",
   "metadata": {},
   "source": [
    "## Step 3: Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a483f66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration (Transfer Learning):\n",
      "  Backbone LR (pretrained): 0.0001\n",
      "  Classifier LR (new layers): 0.001\n",
      "  Epochs: 30\n",
      "  Batch size: 32\n",
      "  Optimizer: Adam (with different LRs)\n",
      "  Scheduler: CosineAnnealingLR\n",
      "  Loss: CrossEntropyLoss\n",
      "  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters for transfer learning\n",
    "# Use lower LR for pretrained backbone, higher for new classifier\n",
    "BACKBONE_LR = 0.0001  # Lower LR for pretrained ResNet layers\n",
    "CLASSIFIER_LR = 0.001  # Higher LR for new classifier layers\n",
    "NUM_EPOCHS = 30  # More epochs for fine-tuning\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize optimizer and scheduler (after model is created)\n",
    "if 'model' in locals():\n",
    "    # Use different learning rates for backbone and classifier\n",
    "    # This is a common practice in transfer learning\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.backbone.parameters(), 'lr': BACKBONE_LR},\n",
    "        {'params': [p for n, p in model.named_parameters() if 'backbone' not in n], 'lr': CLASSIFIER_LR}\n",
    "    ], weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Use cosine annealing for smoother learning rate decay\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "    \n",
    "    print(\"Training configuration (Transfer Learning):\")\n",
    "    print(f\"  Backbone LR (pretrained): {BACKBONE_LR}\")\n",
    "    print(f\"  Classifier LR (new layers): {CLASSIFIER_LR}\")\n",
    "    print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  Optimizer: Adam (with different LRs)\")\n",
    "    print(f\"  Scheduler: CosineAnnealingLR\")\n",
    "    print(f\"  Loss: CrossEntropyLoss\")\n",
    "    print(f\"  Device: {device}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the model initialization cell first!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b6f124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Found old model file with incompatible architecture (custom CNN).\n",
      "   Backing up old model and will train new ResNet50 model...\n",
      "   ‚úÖ Old model backed up to: art_classifier_model_old_CNN.pth\n"
     ]
    }
   ],
   "source": [
    "# Check for old model file and handle architecture mismatch\n",
    "model_file = 'art_classifier_model.pth'\n",
    "if os.path.exists(model_file):\n",
    "    try:\n",
    "        # Try to load and check if it's compatible\n",
    "        checkpoint = torch.load(model_file, map_location=device, weights_only=False)\n",
    "        saved_keys = set(checkpoint.get('model_state_dict', {}).keys())\n",
    "        \n",
    "        # Check if it's the old architecture (has conv1, conv2, etc.) or new (has backbone)\n",
    "        if 'conv1.weight' in saved_keys and 'backbone.0.weight' not in saved_keys:\n",
    "            print(\"‚ö†Ô∏è  Found old model file with incompatible architecture (custom CNN).\")\n",
    "            print(\"   Backing up old model and will train new ResNet50 model...\")\n",
    "            import shutil\n",
    "            backup_name = 'art_classifier_model_old_CNN.pth'\n",
    "            if not os.path.exists(backup_name):\n",
    "                shutil.move(model_file, backup_name)\n",
    "                print(f\"   ‚úÖ Old model backed up to: {backup_name}\")\n",
    "            else:\n",
    "                os.remove(model_file)\n",
    "                print(f\"   ‚úÖ Old model removed (backup already exists)\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Found compatible model file: {model_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error checking model file: {e}\")\n",
    "        print(\"   Will start fresh training...\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No existing model file found. Will train new model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6ad00",
   "metadata": {},
   "source": [
    "## Step 4: Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ca6bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model on test set\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a9f13fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([32, 3, 224, 224])\n",
      "Forward pass OK, output shape: torch.Size([32, 25])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "print(\"Images shape:\", images.shape)  # expect [B, 3, H, W]\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "print(\"Forward pass OK, output shape:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a3c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "\n",
      "Epoch 1 of 30\n",
      "Epoch [1/30]\n",
      "  Train Loss: 2.3196, Train Acc: 33.62%\n",
      "  Val Loss: 1.6919, Val Acc: 46.72%\n",
      "  LR: 0.000100\n",
      "\n",
      "Epoch 2 of 30\n",
      "Error loading image art Folder\\images\\images\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\Albrecht_DuŒì√≤√°‚îú¬¨rer_181.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer_181.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_Du‚ï†√™rer\\Albrecht_Du‚ï†√™rer_8.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_Du‚ï†√™rer\\\\Albrecht_Du‚ï†√™rer_8.jpg'\n",
      "Error loading image art Folder\\images\\images\\Andy_Warhol\\Andy_Warhol_175.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Andy_Warhol\\\\Andy_Warhol_175.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_Du‚ï†√™rer\\Albrecht_Du‚ï†√™rer_207.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_Du‚ï†√™rer\\\\Albrecht_Du‚ï†√™rer_207.jpg'\n",
      "Error loading image art Folder\\images\\images\\Amedeo_Modigliani\\Amedeo_Modigliani_99.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Amedeo_Modigliani\\\\Amedeo_Modigliani_99.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_Du‚ï†√™rer\\Albrecht_Du‚ï†√™rer_128.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_Du‚ï†√™rer\\\\Albrecht_Du‚ï†√™rer_128.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_Du‚ï†√™rer\\Albrecht_Du‚ï†√™rer_234.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_Du‚ï†√™rer\\\\Albrecht_Du‚ï†√™rer_234.jpg'\n",
      "Error loading image art Folder\\images\\images\\Gustav_Klimt\\Gustav_Klimt_41.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Gustav_Klimt\\\\Gustav_Klimt_41.jpg'\n",
      "Error loading image art Folder\\images\\images\\Gustave_Courbet\\Gustave_Courbet_1.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Gustave_Courbet\\\\Gustave_Courbet_1.jpg'\n",
      "Error loading image art Folder\\images\\images\\El_Greco\\El_Greco_32.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\El_Greco\\\\El_Greco_32.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_Du‚ï†√™rer\\Albrecht_Du‚ï†√™rer_108.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_Du‚ï†√™rer\\\\Albrecht_Du‚ï†√™rer_108.jpg'\n",
      "Epoch [2/30]\n",
      "  Train Loss: 1.6596, Train Acc: 49.02%\n",
      "  Val Loss: 1.5492, Val Acc: 51.55%\n",
      "  LR: 0.000099\n",
      "\n",
      "Epoch 3 of 30\n",
      "Error loading image art Folder\\images\\images\\Claude_Monet\\Claude_Monet_40.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Claude_Monet\\\\Claude_Monet_40.jpg'\n",
      "Error loading image art Folder\\images\\images\\Eugene_Delacroix\\Eugene_Delacroix_4.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Eugene_Delacroix\\\\Eugene_Delacroix_4.jpg'\n",
      "Error loading image art Folder\\images\\images\\Henri_Matisse\\Henri_Matisse_65.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Henri_Matisse\\\\Henri_Matisse_65.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\Albrecht_DuŒì√≤√°‚îú¬¨rer_321.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer_321.jpg'\n",
      "Error loading image art Folder\\images\\images\\Eugene_Delacroix\\Eugene_Delacroix_17.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Eugene_Delacroix\\\\Eugene_Delacroix_17.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\Albrecht_DuŒì√≤√°‚îú¬¨rer_19.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer_19.jpg'\n",
      "Error loading image art Folder\\images\\images\\Giotto_di_Bondone\\Giotto_di_Bondone_72.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Giotto_di_Bondone\\\\Giotto_di_Bondone_72.jpg'\n",
      "Error loading image art Folder\\images\\images\\El_Greco\\El_Greco_6.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\El_Greco\\\\El_Greco_6.jpg'\n",
      "Error loading image art Folder\\images\\images\\Gustave_Courbet\\Gustave_Courbet_50.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Gustave_Courbet\\\\Gustave_Courbet_50.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_Du‚ï†√™rer\\Albrecht_Du‚ï†√™rer_39.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_Du‚ï†√™rer\\\\Albrecht_Du‚ï†√™rer_39.jpg'\n",
      "Error loading image art Folder\\images\\images\\Andrei_Rublev\\Andrei_Rublev_32.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Andrei_Rublev\\\\Andrei_Rublev_32.jpg'\n",
      "Error loading image art Folder\\images\\images\\Caravaggio\\Caravaggio_45.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Caravaggio\\\\Caravaggio_45.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\Albrecht_DuŒì√≤√°‚îú¬¨rer_30.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer_30.jpg'\n",
      "Error loading image art Folder\\images\\images\\Edgar_Degas\\Edgar_Degas_666.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Edgar_Degas\\\\Edgar_Degas_666.jpg'\n",
      "Error loading image art Folder\\images\\images\\Henri_Matisse\\Henri_Matisse_102.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Henri_Matisse\\\\Henri_Matisse_102.jpg'\n",
      "Error loading image art Folder\\images\\images\\Edouard_Manet\\Edouard_Manet_19.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Edouard_Manet\\\\Edouard_Manet_19.jpg'\n",
      "Error loading image art Folder\\images\\images\\Amedeo_Modigliani\\Amedeo_Modigliani_19.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Amedeo_Modigliani\\\\Amedeo_Modigliani_19.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_Du‚ï†√™rer\\Albrecht_Du‚ï†√™rer_239.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_Du‚ï†√™rer\\\\Albrecht_Du‚ï†√™rer_239.jpg'\n",
      "Error loading image art Folder\\images\\images\\Andrei_Rublev\\Andrei_Rublev_50.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Andrei_Rublev\\\\Andrei_Rublev_50.jpg'\n",
      "Error loading image art Folder\\images\\images\\Alfred_Sisley\\Alfred_Sisley_118.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Alfred_Sisley\\\\Alfred_Sisley_118.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\Albrecht_DuŒì√≤√°‚îú¬¨rer_187.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer_187.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_Du‚ï†√™rer\\Albrecht_Du‚ï†√™rer_312.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_Du‚ï†√™rer\\\\Albrecht_Du‚ï†√™rer_312.jpg'\n",
      "Error loading image art Folder\\images\\images\\Edgar_Degas\\Edgar_Degas_690.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Edgar_Degas\\\\Edgar_Degas_690.jpg'\n",
      "Error loading image art Folder\\images\\images\\Camille_Pissarro\\Camille_Pissarro_86.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Camille_Pissarro\\\\Camille_Pissarro_86.jpg'\n",
      "Error loading image art Folder\\images\\images\\Gustav_Klimt\\Gustav_Klimt_19.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Gustav_Klimt\\\\Gustav_Klimt_19.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\Albrecht_DuŒì√≤√°‚îú¬¨rer_121.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer_121.jpg'\n",
      "Error loading image art Folder\\images\\images\\Edgar_Degas\\Edgar_Degas_581.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Edgar_Degas\\\\Edgar_Degas_581.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\Albrecht_DuŒì√≤√°‚îú¬¨rer_296.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer_296.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\Albrecht_DuŒì√≤√°‚îú¬¨rer_157.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer_157.jpg'\n",
      "Error loading image art Folder\\images\\images\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\Albrecht_DuŒì√≤√°‚îú¬¨rer_308.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer\\\\Albrecht_DuŒì√≤√°‚îú¬¨rer_308.jpg'\n",
      "Error loading image art Folder\\images\\images\\Edgar_Degas\\Edgar_Degas_283.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Edgar_Degas\\\\Edgar_Degas_283.jpg'\n",
      "Error loading image art Folder\\images\\images\\Edgar_Degas\\Edgar_Degas_282.jpg: [Errno 2] No such file or directory: 'art Folder\\\\images\\\\images\\\\Edgar_Degas\\\\Edgar_Degas_282.jpg'\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "if 'train_loader' in locals() and 'val_loader' in locals() and 'optimizer' in locals():\n",
    "    # if the model is already trained, skip the training loop\n",
    "    # check if the path 'art_classifier_model.pth' exists\n",
    "    if os.path.exists('art_classifier_model.pth'):\n",
    "        print(\"üöÄ Model already trained! Skipping training...\")\n",
    "        # load the model from the path 'art_classifier_model.pth'\n",
    "        model.load_state_dict(torch.load('art_classifier_model.pth')['model_state_dict'])\n",
    "        \n",
    "    else:\n",
    "        print(\"üöÄ Starting training...\\n\")\n",
    "        \n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs = [], []\n",
    "        best_val_acc = 0.0\n",
    "        best_model_state = None\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"Epoch {epoch+1} of {NUM_EPOCHS}\")\n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "            print()\n",
    "        \n",
    "    # Load best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"‚úÖ Training complete! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        \n",
    "        # Test on test set\n",
    "        print(\"\\nüìä Evaluating on test set...\")\n",
    "        test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "        \n",
    "        # Save model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'num_classes': num_classes,\n",
    "            'artists': selected_artists,\n",
    "            'test_acc': test_acc,\n",
    "        }, 'art_classifier_model.pth')\n",
    "        print(\"\\nüíæ Model saved to 'art_classifier_model.pth'\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run the dataset preparation cell first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80368a85",
   "metadata": {},
   "source": [
    "## Training Summary\n",
    "\n",
    "After training, you'll have:\n",
    "- ‚úÖ Trained model saved to `art_classifier_model.pth`\n",
    "- ‚úÖ Model ready for predictions on selected artists\n",
    "- ‚úÖ Embedding layer available for similarity search\n",
    "- ‚úÖ Model ready for interpretability analysis (Captum)\n",
    "\n",
    "**Next steps:** Use the trained model with the interpretability features described in the sections below!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c99bdd",
   "metadata": {},
   "source": [
    "# Artfluence: Art Classification System Architecture\n",
    "\n",
    "This notebook breaks down how to extend the CIFAR-10 tutorial to build a comprehensive art classification system with interpretability features.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The CIFAR-10 tutorial provides the **core engine** (a CNN that outputs logits for classes). Everything else is additional layers of logic on top of that foundation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c1bc4",
   "metadata": {},
   "source": [
    "## Part 1: What the CIFAR-10 Tutorial Already Gives You\n",
    "\n",
    "The CIFAR-10 tutorial trains a CNN, gets raw outputs (\"energies\") for each class, and picks the argmax as the predicted label.\n",
    "\n",
    "### ‚úÖ Directly Supported Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19c836",
   "metadata": {},
   "source": [
    "### 1. Predicted Artist\n",
    "\n",
    "**What it is:** The class with the highest logit value.\n",
    "\n",
    "**How to get it:**\n",
    "- Replace `CIFAR-10 classes = ['airplane', 'car', ...]` with `artists = ['Picasso', 'Matisse', ...]`\n",
    "- Use `torch.max(outputs, 1)` to get the predicted class\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Getting predicted artist from model outputs\n",
    "# Assuming you have a trained model and an input image\n",
    "\n",
    "# artists = ['Picasso', 'Matisse', 'Van Gogh', 'Monet', ...]\n",
    "# outputs = model(input_image)  # Shape: [batch_size, num_artists]\n",
    "\n",
    "# Get predicted artist (argmax)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "# predicted contains the index of the predicted artist\n",
    "\n",
    "# Convert to artist name\n",
    "# predicted_artist = artists[predicted.item()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bb991",
   "metadata": {},
   "source": [
    "### 2. Influence Distribution\n",
    "\n",
    "**What it is:** Probability distribution over all artists showing how much each artist influenced the prediction.\n",
    "\n",
    "**How to get it:**\n",
    "- Apply softmax to the logits to convert them to probabilities\n",
    "- This gives you a probability for every artist = an \"influence distribution\"\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert logits to probability distribution\n",
    "probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "# probs now contains probabilities for each artist\n",
    "# Example output shape: [batch_size, num_artists]\n",
    "# Each row sums to 1.0\n",
    "\n",
    "# You can visualize this as:\n",
    "# - A bar chart showing probability for each artist\n",
    "# - A sorted list of artists by influence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02403a1",
   "metadata": {},
   "source": [
    "### 3. Confidence / Uncertainty Estimates\n",
    "\n",
    "**What it is:** \n",
    "- **Confidence**: How sure the model is about its prediction\n",
    "- **Uncertainty**: How uncertain the model is (opposite of confidence)\n",
    "\n",
    "**How to get it:**\n",
    "- **Confidence** = `max(probs)` for the chosen artist\n",
    "- **Uncertainty** = entropy of probs or `1 - confidence`\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec729cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Confidence - Mean: 0.043, Min: 0.042, Max: 0.043\n",
      "Uncertainty (entropy) - Mean: 0.823, Min: 0.823, Max: 0.823\n",
      "Uncertainty (simple) - Mean: 0.957, Min: 0.957, Max: 0.958\n",
      "\n",
      "First sample in batch:\n",
      "  Confidence: 0.043\n",
      "  Uncertainty (entropy): 0.823\n",
      "  Uncertainty (simple): 0.957\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "artists = pd.read_csv('art Folder/artists.csv')\n",
    "\n",
    "# Get confidence (probability of predicted class)\n",
    "# Note: probs can be a batch, so confidence will be a tensor with shape [batch_size]\n",
    "confidence = torch.max(probs, dim=1)[0]  # Max probability\n",
    "\n",
    "# Get uncertainty using entropy\n",
    "# Higher entropy = more uncertainty (probabilities are spread out)\n",
    "# Lower entropy = less uncertainty (one probability dominates)\n",
    "entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)\n",
    "uncertainty = entropy / torch.log(torch.tensor(len(artists), dtype=torch.float32))  # Normalized\n",
    "\n",
    "# Alternative: Simple uncertainty as 1 - confidence\n",
    "uncertainty_simple = 1 - confidence\n",
    "\n",
    "# Handle both single samples and batches\n",
    "if confidence.numel() == 1:\n",
    "    # Single sample\n",
    "    print(f\"Confidence: {confidence.item():.3f}\")\n",
    "    print(f\"Uncertainty (entropy): {uncertainty.item():.3f}\")\n",
    "    print(f\"Uncertainty (simple): {uncertainty_simple.item():.3f}\")\n",
    "else:\n",
    "    # Batch - show statistics\n",
    "    print(f\"Batch size: {confidence.shape[0]}\")\n",
    "    print(f\"Confidence - Mean: {confidence.mean().item():.3f}, Min: {confidence.min().item():.3f}, Max: {confidence.max().item():.3f}\")\n",
    "    print(f\"Uncertainty (entropy) - Mean: {uncertainty.mean().item():.3f}, Min: {uncertainty.min().item():.3f}, Max: {uncertainty.max().item():.3f}\")\n",
    "    print(f\"Uncertainty (simple) - Mean: {uncertainty_simple.mean().item():.3f}, Min: {uncertainty_simple.min().item():.3f}, Max: {uncertainty_simple.max().item():.3f}\")\n",
    "    \n",
    "    # Show first sample in batch as example\n",
    "    print(f\"\\nFirst sample in batch:\")\n",
    "    print(f\"  Confidence: {confidence[0].item():.3f}\")\n",
    "    print(f\"  Uncertainty (entropy): {uncertainty[0].item():.3f}\")\n",
    "    print(f\"  Uncertainty (simple): {uncertainty_simple[0].item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e0be2",
   "metadata": {},
   "source": [
    "## Part 2: Features Requiring Small Design Extensions\n",
    "\n",
    "These features aren't in the CIFAR-10 tutorial but can be added with standard techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10181872",
   "metadata": {},
   "source": [
    "### 4. Top-K Nearest Paintings\n",
    "\n",
    "**What it is:** Find the k most similar paintings in your database to the query painting.\n",
    "\n",
    "**How to implement:**\n",
    "1. Modify the model's `forward()` method to return embeddings from the penultimate layer (e.g., `fc2`)\n",
    "2. Pre-compute and store embeddings for all paintings in your database\n",
    "3. For a query painting, compute its embedding and do k-NN search (cosine or Euclidean distance)\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87b202b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Modify model to return embeddings\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ArtClassifier(nn.Module):\n",
    "    def __init__(self, num_artists):\n",
    "        super().__init__()\n",
    "        # ... CNN layers ...\n",
    "        self.fc1 = nn.Linear(14 * 14 * 512, 512)  # First FC layer after CNN\n",
    "        self.fc2 = nn.Linear(512, 256)  # Penultimate layer (embedding)\n",
    "        self.fc3 = nn.Linear(256, num_artists)  # Final classification layer\n",
    "    \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        # ... CNN forward pass (conv layers, pooling, flattening) ...\n",
    "        # After CNN: x has shape [batch_size, 14*14*512]\n",
    "        features = F.relu(self.fc1(x))  # Process through first FC layer\n",
    "        embedding = F.relu(self.fc2(features))  # Get embedding from penultimate layer\n",
    "        logits = self.fc3(embedding)  # Get logits from final layer\n",
    "        \n",
    "        if return_embedding:\n",
    "            return logits, embedding\n",
    "        return logits\n",
    "\n",
    "# Step 2: Pre-compute embeddings for database\n",
    "def build_embedding_database(model, dataloader, device):\n",
    "    \"\"\"Pre-compute embeddings for all paintings in database\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    painting_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, ids in dataloader:\n",
    "            images = images.to(device)\n",
    "            _, emb = model(images, return_embedding=True)\n",
    "            embeddings.append(emb.cpu())\n",
    "            painting_ids.extend(ids)\n",
    "    \n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    return embeddings, painting_ids\n",
    "\n",
    "# Step 3: Find top-k nearest paintings\n",
    "def find_top_k_nearest(query_embedding, database_embeddings, k=5, metric='cosine'):\n",
    "    \"\"\"Find k nearest paintings using cosine or Euclidean distance\"\"\"\n",
    "    if metric == 'cosine':\n",
    "        # Normalize embeddings\n",
    "        query_norm = F.normalize(query_embedding, p=2, dim=1)\n",
    "        db_norm = F.normalize(database_embeddings, p=2, dim=1)\n",
    "        # Compute cosine similarity\n",
    "        similarities = torch.mm(query_norm, db_norm.t())\n",
    "        top_k_values, top_k_indices = torch.topk(similarities, k, dim=1)\n",
    "    else:  # Euclidean\n",
    "        distances = torch.cdist(query_embedding, database_embeddings)\n",
    "        top_k_values, top_k_indices = torch.topk(-distances, k, dim=1)  # Negative for top-k\n",
    "    \n",
    "    return top_k_indices, top_k_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea4d21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3865fb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Flagged as Unknown Artist\n",
      "Sample 1: Flagged as Unknown Artist\n",
      "Sample 2: Flagged as Unknown Artist\n",
      "Sample 3: Flagged as Unknown Artist\n",
      "Sample 4: Flagged as Unknown Artist\n",
      "Sample 5: Flagged as Unknown Artist\n",
      "Sample 6: Flagged as Unknown Artist\n",
      "Sample 7: Flagged as Unknown Artist\n",
      "Sample 8: Flagged as Unknown Artist\n",
      "Sample 9: Flagged as Unknown Artist\n",
      "Sample 10: Flagged as Unknown Artist\n",
      "Sample 11: Flagged as Unknown Artist\n",
      "Sample 12: Flagged as Unknown Artist\n",
      "Sample 13: Flagged as Unknown Artist\n",
      "Sample 14: Flagged as Unknown Artist\n",
      "Sample 15: Flagged as Unknown Artist\n",
      "Sample 16: Flagged as Unknown Artist\n",
      "Sample 17: Flagged as Unknown Artist\n",
      "Sample 18: Flagged as Unknown Artist\n",
      "Sample 19: Flagged as Unknown Artist\n",
      "Sample 20: Flagged as Unknown Artist\n",
      "Sample 21: Flagged as Unknown Artist\n",
      "Sample 22: Flagged as Unknown Artist\n",
      "Sample 23: Flagged as Unknown Artist\n",
      "Sample 24: Flagged as Unknown Artist\n",
      "Sample 25: Flagged as Unknown Artist\n",
      "Sample 26: Flagged as Unknown Artist\n",
      "Sample 27: Flagged as Unknown Artist\n",
      "Sample 28: Flagged as Unknown Artist\n",
      "Sample 29: Flagged as Unknown Artist\n",
      "Sample 30: Flagged as Unknown Artist\n",
      "Sample 31: Flagged as Unknown Artist\n"
     ]
    }
   ],
   "source": [
    "# Simple threshold-based approach\n",
    "def is_unknown_artist(probs, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Flag as unknown if confidence is too low\n",
    "    Returns: tensor of booleans (one per sample in batch)\n",
    "    \"\"\"\n",
    "    confidence = torch.max(probs, dim=1)[0]\n",
    "    return confidence < confidence_threshold\n",
    "\n",
    "# Example usage\n",
    "probs = torch.softmax(outputs, dim=1)\n",
    "unknown_flags = is_unknown_artist(probs, confidence_threshold=0.3)\n",
    "\n",
    "# Handle both single samples and batches\n",
    "if unknown_flags.numel() == 1:\n",
    "    # Single sample\n",
    "    if unknown_flags.item():\n",
    "        print(\"Flagged as: Unknown Artist\")\n",
    "    else:\n",
    "        predicted_idx = torch.argmax(probs, dim=1)\n",
    "        print(f\"Predicted: {artists[predicted_idx.item()]}\")\n",
    "else:\n",
    "    # Batch - process each sample\n",
    "    predicted_indices = torch.argmax(probs, dim=1)\n",
    "    for i in range(len(unknown_flags)):\n",
    "        if unknown_flags[i].item():\n",
    "            print(f\"Sample {i}: Flagged as Unknown Artist\")\n",
    "        else:\n",
    "            print(f\"Sample {i}: Predicted {artists[predicted_indices[i].item()]}\")\n",
    "\n",
    "# Better: OOD detection using embedding distance\n",
    "def is_unknown_ood(query_embedding, database_embeddings, threshold_percentile=95):\n",
    "    \"\"\"\n",
    "    Flag as unknown if query embedding is far from all known artist embeddings\n",
    "    Uses percentile of distances as threshold\n",
    "    \"\"\"\n",
    "    # Compute distances to all database embeddings\n",
    "    distances = torch.cdist(query_embedding, database_embeddings)\n",
    "    min_distance = torch.min(distances, dim=1)[0]\n",
    "    \n",
    "    # Threshold: if min distance > 95th percentile of all distances, it's OOD\n",
    "    threshold = torch.quantile(distances, threshold_percentile / 100.0)\n",
    "    \n",
    "    return min_distance > threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774244e2",
   "metadata": {},
   "source": [
    "## Part 3: Interpretability Features (Requires Additional Tooling)\n",
    "\n",
    "These features go beyond the CIFAR-10 tutorial and require **Captum** (PyTorch's interpretability library) and additional image analysis code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16d9f0",
   "metadata": {},
   "source": [
    "### 6. Per-Artist Factor Explanation\n",
    "\n",
    "**What it is:** Understand which visual regions/features the model focuses on when predicting a specific artist.\n",
    "\n",
    "**How to implement:**\n",
    "- Use **Captum** methods like Integrated Gradients, Grad-CAM, or Guided Backpropagation\n",
    "- Generate heatmaps showing which pixels/regions contributed most to the prediction\n",
    "- Summarize as: *\"For Picasso, the model focused on high-contrast angular shapes in the upper left and bold outlines in faces\"*\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b19f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Captum: pip install captum\n",
    "\n",
    "from captum.attr import IntegratedGradients, GradCAM, GuidedBackprop\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "# Initialize attribution methods\n",
    "integrated_gradients = IntegratedGradients(model)\n",
    "grad_cam = GradCAM(model, model.layer4)  # Use appropriate layer\n",
    "guided_backprop = GuidedBackprop(model)\n",
    "\n",
    "# Get attributions for a specific artist prediction\n",
    "def get_artist_attributions(model, input_image, target_artist_idx):\n",
    "    \"\"\"\n",
    "    Get attribution maps showing which pixels contributed to predicting a specific artist\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Method 1: Integrated Gradients\n",
    "    attributions_ig = integrated_gradients.attribute(\n",
    "        input_image, \n",
    "        target=target_artist_idx,\n",
    "        n_steps=50\n",
    "    )\n",
    "    \n",
    "    # Method 2: Grad-CAM (class activation maps)\n",
    "    attributions_gradcam = grad_cam.attribute(\n",
    "        input_image,\n",
    "        target=target_artist_idx\n",
    "    )\n",
    "    \n",
    "    # Method 3: Guided Backpropagation\n",
    "    attributions_gbp = guided_backprop.attribute(\n",
    "        input_image,\n",
    "        target=target_artist_idx\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'integrated_gradients': attributions_ig,\n",
    "        'gradcam': attributions_gradcam,\n",
    "        'guided_backprop': attributions_gbp\n",
    "    }\n",
    "\n",
    "# Visualize attributions\n",
    "def visualize_attributions(input_image, attributions, artist_name):\n",
    "    \"\"\"Visualize which regions contributed to the artist prediction\"\"\"\n",
    "    # Convert to numpy for visualization\n",
    "    input_np = input_image.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    attr_np = attributions.squeeze().cpu().detach().numpy()\n",
    "    \n",
    "    # Use Captum's visualization\n",
    "    viz.visualize_image_attr(\n",
    "        attr_np,\n",
    "        input_np,\n",
    "        method=\"heat_map\",\n",
    "        sign=\"all\",\n",
    "        title=f\"Attribution for {artist_name}\"\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "# predicted_idx = torch.argmax(outputs, dim=1).item()\n",
    "# attributions = get_artist_attributions(model, input_image, predicted_idx)\n",
    "# visualize_attributions(input_image, attributions['gradcam'], artists[predicted_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8d6b0",
   "metadata": {},
   "source": [
    "### 7. Explanation of Visual Elements (Colors, Texture, Brush Strokes)\n",
    "\n",
    "**What it is:** Explain predictions in terms of human-understandable visual features like colors, brush stroke thickness, and texture.\n",
    "\n",
    "**Reality Check:**\n",
    "- **Colors**: Easy to quantify (RGB channels, color histograms, attribution across color channels)\n",
    "- **Texture/Brush Strokes**: Detectable qualitatively via CNN filters and attribution maps, but won't output exact \"brush thickness = 4px\"\n",
    "\n",
    "**How to implement:**\n",
    "1. Combine attribution maps with image statistics\n",
    "2. Analyze which color channels/regions get high attribution\n",
    "3. Inspect early conv filters that respond to edges/strokes\n",
    "4. Generate human-readable descriptions\n",
    "\n",
    "**Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf415b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def analyze_colors(input_image, attributions):\n",
    "    \"\"\"\n",
    "    Analyze which colors contributed most to the prediction\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    img_np = input_image.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    attr_np = attributions.squeeze().cpu().detach().numpy()\n",
    "    \n",
    "    # Get color channels\n",
    "    r, g, b = img_np[:, :, 0], img_np[:, :, 1], img_np[:, :, 2]\n",
    "    \n",
    "    # Weight by attributions\n",
    "    r_weighted = np.sum(r * attr_np)\n",
    "    g_weighted = np.sum(g * attr_np)\n",
    "    b_weighted = np.sum(b * attr_np)\n",
    "    \n",
    "    # Determine dominant color influence\n",
    "    total = r_weighted + g_weighted + b_weighted\n",
    "    color_contributions = {\n",
    "        'red': r_weighted / total,\n",
    "        'green': g_weighted / total,\n",
    "        'blue': b_weighted / total\n",
    "    }\n",
    "    \n",
    "    # Convert to hue description\n",
    "    dominant_hue = max(color_contributions, key=color_contributions.get)\n",
    "    \n",
    "    return {\n",
    "        'dominant_color': dominant_hue,\n",
    "        'contributions': color_contributions,\n",
    "        'description': f\"Model focused on {dominant_hue} tones\"\n",
    "    }\n",
    "\n",
    "def analyze_texture_brush_strokes(input_image, attributions):\n",
    "    \"\"\"\n",
    "    Analyze texture and brush stroke patterns (qualitative)\n",
    "    \"\"\"\n",
    "    img_np = input_image.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    attr_np = attributions.squeeze().cpu().detach().numpy()\n",
    "    \n",
    "    # Convert to grayscale for texture analysis\n",
    "    gray = cv2.cvtColor((img_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Edge detection (rough proxy for brush strokes)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = np.sum(edges > 0) / edges.size\n",
    "    \n",
    "    # Weight edges by attributions\n",
    "    attr_resized = cv2.resize(attr_np, (gray.shape[1], gray.shape[0]))\n",
    "    edge_attribution = np.sum(edges * attr_resized) / np.sum(attr_resized + 1e-10)\n",
    "    \n",
    "    # Texture analysis using local variance\n",
    "    kernel = np.ones((5, 5), np.float32) / 25\n",
    "    local_mean = cv2.filter2D(gray.astype(np.float32), -1, kernel)\n",
    "    local_var = cv2.filter2D((gray.astype(np.float32) - local_mean)**2, -1, kernel)\n",
    "    texture_roughness = np.mean(local_var)\n",
    "    \n",
    "    # Generate description\n",
    "    if edge_attribution > 0.5 and texture_roughness > 1000:\n",
    "        description = \"Energetic, visible brush strokes with high texture\"\n",
    "    elif edge_attribution > 0.3:\n",
    "        description = \"Moderate brush stroke visibility\"\n",
    "    else:\n",
    "        description = \"Smooth, fine brushwork\"\n",
    "    \n",
    "    return {\n",
    "        'edge_density': edge_density,\n",
    "        'texture_roughness': texture_roughness,\n",
    "        'brush_stroke_attribution': edge_attribution,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "def generate_visual_explanation(input_image, attributions, artist_name):\n",
    "    \"\"\"\n",
    "    Generate human-readable explanation combining all visual factors\n",
    "    \"\"\"\n",
    "    color_analysis = analyze_colors(input_image, attributions)\n",
    "    texture_analysis = analyze_texture_brush_strokes(input_image, attributions)\n",
    "    \n",
    "    explanation = f\"\"\"\n",
    "    For {artist_name}:\n",
    "    - Color: {color_analysis['description']}\n",
    "    - Texture: {texture_analysis['description']}\n",
    "    - Dominant color influence: {color_analysis['dominant_color']}\n",
    "    \"\"\"\n",
    "    \n",
    "    return explanation\n",
    "\n",
    "# Example usage\n",
    "# attributions = get_artist_attributions(model, input_image, predicted_idx)['gradcam']\n",
    "# explanation = generate_visual_explanation(input_image, attributions, artists[predicted_idx])\n",
    "# print(explanation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbfb51f",
   "metadata": {},
   "source": [
    "## Part 4: Architecture Overview\n",
    "\n",
    "### Complete Artfluence Pipeline\n",
    "\n",
    "```\n",
    "Input Painting\n",
    "    ‚Üì\n",
    "[CNN Feature Extraction]\n",
    "    ‚Üì\n",
    "[Penultimate Layer (fc2)] ‚Üí Embeddings ‚Üí Top-K Nearest Paintings\n",
    "    ‚Üì\n",
    "[Final Layer (fc3)] ‚Üí Logits\n",
    "    ‚Üì\n",
    "[Softmax] ‚Üí Probabilities (Influence Distribution)\n",
    "    ‚Üì\n",
    "[Argmax] ‚Üí Predicted Artist\n",
    "    ‚Üì\n",
    "[Confidence Check] ‚Üí Unknown Artist Flag (if confidence < threshold)\n",
    "    ‚Üì\n",
    "[Captum Attribution] ‚Üí Per-Artist Factor Explanations\n",
    "    ‚Üì\n",
    "[Visual Analysis] ‚Üí Color/Texture/Brush Stroke Explanations\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Core Model**: CNN architecture from CIFAR-10 tutorial (modified for artists)\n",
    "2. **Embedding Layer**: Penultimate layer outputs for similarity search\n",
    "3. **Classification Layer**: Final layer outputs logits for artist prediction\n",
    "4. **Post-Processing**: Softmax, confidence, uncertainty calculations\n",
    "5. **Interpretability**: Captum for attribution maps\n",
    "6. **Visual Analysis**: Custom functions for color/texture analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db3985",
   "metadata": {},
   "source": [
    "## Summary: What's Possible Based on CIFAR-10 Tutorial\n",
    "\n",
    "### ‚úÖ Directly Supported (from tutorial):\n",
    "- Train a CNN on images\n",
    "- Get logits, pick predicted class (artist)\n",
    "- Turn logits into probability distribution (influence distribution) with softmax\n",
    "- Derive confidence/uncertainty from probabilities\n",
    "\n",
    "### ‚úÖ Requires Small Extensions:\n",
    "- **Top-k nearest paintings**: Use penultimate-layer embeddings + k-NN search\n",
    "- **Unknown artist flag**: Thresholding or OOD detection on logits/embeddings\n",
    "\n",
    "### ‚ö†Ô∏è Requires Additional Tooling (Captum + custom code):\n",
    "- **Per-artist factor explanations**: Feature attribution methods (Grad-CAM, Integrated Gradients)\n",
    "- **Visual element explanations**: Colors (easy), textures/brush strokes (qualitative inference)\n",
    "\n",
    "### üìù Notes:\n",
    "- The CIFAR-10 tutorial gives you the **right architectural starting point**\n",
    "- It does **not** implement all interpretability features by itself\n",
    "- Those features are **doable** but require the next layer: **Captum + your own analysis code**\n",
    "- Brush stroke \"thickness\" won't be perfect‚Äîit's qualitative inference from texture/edge patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e72d5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Install Captum**: `pip install captum`\n",
    "2. **Modify CIFAR-10 model** to return embeddings\n",
    "3. **Build embedding database** for your painting collection\n",
    "4. **Implement k-NN search** for top-k nearest paintings\n",
    "5. **Add Captum attribution** methods for interpretability\n",
    "6. **Create visual analysis functions** for colors/textures\n",
    "7. **Combine everything** into the complete Artfluence pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f692710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
